## Welcome to my homepage!

Welcome! I am Ning Dai, a third-year PhD student in the School of Electrical Engineering and Computer Science at **Oregon State University**. I am privileged to work under the supervision of [Prof. Liang Huang](https://web.engr.oregonstate.edu/~huanlian/){:target="_blank"}. 

Prior to my PhD journey, I served as an undergraduate research assistant with the **FudanNLP** Group, under the mentorship of [Prof. Xipeng Qiu](https://xpqiu.github.io/en.html){:target="_blank"}.  





## Research Interests

My research is anchored in the intersections of **Natural Language Processing (NLP)**, **Machine Learning (ML)**, and **Deep Learning (DL)**. My journey through these fields has led me to explore a spectrum of topics within these areas, ranging from Reinforcement Learning, Syntactic Parsing, Text Style Transfer and Self-supervised Representation Learning. As I continue to delve deeper into these domains, my current research focuses have shifted and expanded as follows:

- **Text Generation & Generative Models for Sequential Data**

	Here, I study generative models and their algorithms that span across NLP and Computational Biology. For NLP, I'm exploring models that can produce coherent, contextually-relevant, and diverse textual outputs. In Computational Biology, I'm interested in how generative models can simulate biological sequences, predict structures, or design novel sequences for therapeutic applications.

- **Fine-grained Human Supervision for Aligning Large Language Models**

  While Large Language Models possess groundbreaking potential, ensuring their alignment with nuanced human perceptions and intentions is still challenging. My research in this space revolves around leveraging fine-grained human supervision through reinforcement learning. This approach aims to steer LLMs, enhancing their adaptability and performance in various applications.

- **Efficient ML Algorithms & DL Models for Computational Biology**
	
	The nexus of ML and computational biology unfolds a rich tapestry of challenges and prospects. I am particularly focusing on challenges in RNA folding and design problems. Here, pioneering ML algorithms and DL models stand to provide trailblazing solutions, offer unprecedented insights, potentially reshaping our understanding of complex biological matrices.
	


## Publications


------

*<font face ="Times New Roman" size="4">LinearCoFold and LinearCoPartition: linear-time algorithms for secondary structure prediction of interacting RNA molecules</font>* (Nucleic Acids Research)  [[paper]](https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkad664/7256890){:target="_blank"}  

He Zhang, Sizhen Li, **Ning Dai**, Liang Zhang, David H Mathews, Liang Huang


------

*<font face ="Times New Roman" size="4">RNA design via structure-aware multifrontier ensemble optimization</font>* (Bioinformatics)  [[paper]](https://academic.oup.com/bioinformatics/article/39/Supplement_1/i563/7210514){:target="_blank"}  

Tianshuo Zhou, **Ning Dai**, Sizhen Li, Max Ward, David H Mathews, Liang Huang

------

*<font face ="Times New Roman" size="4">LinearSankoff: Linear-time Simultaneous Folding and Alignment of RNA Homologs</font>* (arXiv preprint)  [[paper]](https://arxiv.org/pdf/2307.09580.pdf){:target="_blank"}  

Sizhen Li, **Ning Dai**, He Zhang, Apoorv Malik, David H. Mathews, Liang Huang

------

*<font face ="Times New Roman" size="4">Style Transformer:  Unpaired Text Style Transfer without Disentangled Latent Representation</font>* (ACL 2019)  [[paper]](https://www.aclweb.org/anthology/P19-1601.pdf){:target="_blank"}  [[code]](https://github.com/fastnlp/style-transformer){:target="_blank"}  

**Ning Dai**, Jianze Liang, Xipeng Qiu, Xuanjing Huang

------

*<font face ="Times New Roman" size="4">A Plan-and-Pretrain Approach for Knowledge Graph-to-Text Generation</font>*   [[paper]](https://aclanthology.org/2020.webnlg-1.10.pdf){:target="_blank"} 

Qipeng Guo, Zhijing Jin, **Ning Dai**, Xipeng Qiu, Xiangyang Xue, David Wipf, Zheng Zhang

------

*<font face ="Times New Roman" size="4">Pre-trained Models for Natural Language Processing: A Survey</font>*   [[paper]](https://arxiv.org/pdf/2003.08271.pdf){:target="_blank"} 

Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, **Ning Dai**, Xuanjing Huang

------






## Experiences


**Tencent AI Lab Seattle**  <img src="./assets/img/Tencent_logo.png" align='right'> 

*<font face ="Times New Roman">Jul 2023 ~ Sept 2023</font>*     ‖  Research Intern

------

**Baidu Research USA**  <img src="./assets/img/Baidu_logo.png" align='right'> 

*<font face ="Times New Roman">Jul 2022 ~ Sept 2022</font>*     ‖  Research Intern

------

**ByteDance AI Lab**  <img src="./assets/img/ByteDance_logo.png" align='right'> 

*<font face ="Times New Roman">Sept 2020 ~ Aug 2021</font>*     ‖  Research Intern,  MLNLC  Group

------

**AWS Shanghai AI Lab**  <img src='./assets/img/aws_logo.png' align='right'> 

*<font face ="Times New Roman">Nov 2019 ~ Aug 2020</font>*     ‖  Applied Scientist Intern,  [Prof. Zheng Zhang](https://shanghai.nyu.edu/academics/faculty/directory/zheng-zhang){:target="_blank"}'s Group

------

**Montreal Institute for Learning Algorithms (Mila)**  <img src='./assets/img/mila_logo.png' align='right'>

*<font face ="Times New Roman">Jul 2019 ~ Oct 2019</font>* ‖  Research Intern,  [Prof. Christopher Pal](https://mila.quebec/en/person/pal-christopher/){:target="_blank"}'s Group

------

